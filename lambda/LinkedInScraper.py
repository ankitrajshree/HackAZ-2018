import bs4 as bs4import jsonimport requestsimport timedef getJobDetails(sess, url):    #r = requests.get(url)    r = sess.get(url)    data = r.text    soup = bs4.BeautifulSoup(data, features="html.parser")    import re    q = re.compile(r"(\"og:(.*)\" .*=\"(.*)\">)")    matchDict = {}    #print(soup)    matches = re.findall(q, data)    for match in matches:        try:            matchDict[match[1]] = match[2]            pass        except:            pass    return matchDictdef get_dict_from_url(sess, url ,codeID):    """    returns a dictionary from any data found in the first html 'code' element with given id in given url.    requires a valid linkedIn session, sess.    """    resp=sess.get(url)    #time.sleep(4)    soup = bs4.BeautifulSoup(resp.text, features="html.parser")    target= soup.find('code',{ 'id': codeID})    return json.loads(target.contents[0]) if target else {'description':''}def LinkedInScrape():    # set credentials    login = {'session_key': 'rishabh.zn201@gmail.com'        , 'session_password': 'honolulu'}  # passwd.passwd}    # start linkedin Session    URL = 'https://www.linkedin.com/uas/login-submit'    sess = requests.session()    #time.sleep(1)    sess.post(URL, data=login)    #time.sleep(1)    #hack    str = 'https://www.linkedin.com/jobs/view/st-govt-intern-%28data-management%29-at-state-of-arizona-559796837/'    r = requests.get(str)    #hack    search = {        'Engineer': 'https://www.linkedin.com/jobs/search/?keywords=Data%20Intern&location=Tempe%2C%20Arizona'}  # &locationId=PLACES.us.4-1-0-8-33'}    pages = get_dict_from_url(sess, search['Engineer'], 'decoratedJobPostingsModule')    #Step 2:    try:        if pages['description'] == '':            #print(pages)            #print("Not Found")            return {'list_count': 0, 'list': []}    except:        #print(pages)        pass    results = pages['elements']    jsonObject = {'list_count': 0, 'list': []}    jobDetailsList = []    for result in results:        url = result['viewJobCanonicalUrl']        #print(url)        jobDetailsList.append(getJobDetails(sess, url))    job_dict_json = json.dumps(jobDetailsList)    jsonObject['list_count'] = sum([1 for i in jobDetailsList])    jsonObject['list'] = jobDetailsList    #print(results)    sess.close()    return jsonObjectdef lambda_handler(event, context):    print(event)    print(context)    return LinkedInScrape()    #c = 20#print(LinkedInScrape())print(lambda_handler("",""))